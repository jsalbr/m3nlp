{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Question_Answering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsalbr/m3nlp/blob/main/Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiAcHrZPrBpg"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJxDfaburBpm"
      },
      "source": [
        "Dieses Notebook sollte mit GPU ausgef√ºhrt werden.  \n",
        "Daf√ºr bitte zun√§chst im Men√º \"Laufzeit\"->\"Laufzeittyp √§ndern\"->\"Hardwarebeschleuniger: GPU\" einstellen.\n",
        "\n",
        "\n",
        "Credits: Das Notebook verwendet Ideen von\n",
        "  * Natural Language Processing with Transformers von Lewis Tunstall, Leandro von Werra, Thomas Wolf, O'Reilly 2021, https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n",
        "  * Heise Academy NLP-Kurs von Christian Winkler, https://github.com/heiseacademy/nlp-course/tree/main/09_Transfer_Learning\n",
        "  * Haystack Tutorial von deepset.io, https://github.com/deepset-ai/haystack#mortar_board-tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtRDsDqrBpn"
      },
      "source": [
        "## System vorbereiten\n",
        "\n",
        "### Installation von Transformers und Haystack\n",
        "\n",
        "Achtung: In diesem Notebook werden sowohl die [Transformers-Bibliothek von HuggingFace](https://huggingface.co/transformers/) als auch [Haystack von deepset.ai](https://haystack.deepset.ai/) eingesetzt. \n",
        "\n",
        "Leider haben in der aktuellen Version beide Bibliotheken inkompatible Dependencies.\n",
        "Die Installation hier funktioniert f√ºr diese Beispiele, aber es gibt eine Warnung am Ende. In der Praxis kann\n",
        "es daher zu Problemen kommen. F√ºr produktive Zwecke sollte deshalb mit getrennten\n",
        "virtuellen Environments arbeiten.\n",
        "\n",
        "**Geduld:** Die Installation dauert einen Moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbZyqQ-zqxbh"
      },
      "source": [
        "!pip install farm-haystack==0.10.0 grpcio==1.41.0\n",
        "!pip install transformers==4.12.3 datasets\n",
        "!pip install readability-lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQD3jhTVcBVe"
      },
      "source": [
        "# patch for transformers 4.12.3, see https://github.com/huggingface/transformers/issues/14311\n",
        "!wget https://raw.githubusercontent.com/jsalbr/m3nlp/main/question_answering.patch\n",
        "!patch /usr/local/lib/python3.7/dist-packages/transformers/pipelines/question_answering.py -i question_answering.patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_Q7Cacd2ID"
      },
      "source": [
        "!patch /usr/local/lib/python3.7/dist-packages/transformers/pipelines/question_answering.py -i question_answering.patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPTt9iVkdzo0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0JuwR2_Vui"
      },
      "source": [
        "### Noch ein paar Standard-Einstellungen setzen ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82D4tSavG7U1"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "from textwrap import wrap, fill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYWJh8F7T8ZY"
      },
      "source": [
        "# suppress warnings\n",
        "import warnings;\n",
        "warnings.filterwarnings('ignore');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui6-tKDZvThZ"
      },
      "source": [
        "### Und eine kleine Anzeige-Funktion ...\n",
        "\n",
        "welche mit Antworten sowohl von Transformer als auch von Haystack umgehen kann."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3SVM0ZIZcuS"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_qa(answers, question='', context='', padding=50):\n",
        "    if type(answers) != list:\n",
        "        answers = [answers]\n",
        "    html = \"<table>\"\n",
        "    if len(question) > 0: \n",
        "        html += f\"<tr><td>Question:</td><td><span style='font-weight:bold'>{question}</span></td></tr>\"\n",
        "        html += f\"<tr><td>&nbsp;<td><td> </td></tr>\"\n",
        "    for a in answers:\n",
        "        if len(a['answer']) > 0:\n",
        "            html += f\"<tr><td>Answer:</td><td><span style='font-weight:bold'>{a['answer']}</span></td></tr>\"\n",
        "        else:\n",
        "            html += f\"<tr><td>Answer:</td><td>answer impossible</td></tr>\"\n",
        "        html += f\"<tr><td>Score:</td><td>{a['score']}</td></tr>\"\n",
        "        start = a.get('start', a.get('offset_start'))\n",
        "        end = a.get('end', a.get('offset_end'))\n",
        "        html += f\"<tr><td>Span:</td><td>{start}:{end}</td></tr>\"\n",
        "        ctx = a.get('context', context)\n",
        "        if len(a['answer']) > 0 and len(ctx) > 0:\n",
        "            left = max(0, start-padding)\n",
        "            right = min(end+padding, len(ctx))\n",
        "            html += \"<tr><td>Snippet:</td><td>\"\n",
        "            html += f\"{ctx[left:start]}<span style='color:blue;font-weight:bold'>\"\n",
        "            html += ctx[start:end]\n",
        "            html += f\"</span>{ctx[end:right]}</td>\"\n",
        "        html += f\"<tr><td>&nbsp;<td><td> </td></tr>\"\n",
        "    html += '</table><br/>'\n",
        "    display(HTML(html))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UycdkUQf-nHR"
      },
      "source": [
        "## Arbeit mit einem QA-Modell\n",
        "\n",
        "Zun√§chst nutzen wir die [HuggingFace Transformers Library](https://huggingface.co/transformers/), um mit einem vortrainierten QA-Modell zu arbeiten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixsWFe1JKE8Z"
      },
      "source": [
        "### Modell laden\n",
        "\n",
        "Eine √úbersicht √ºber die QA-Modelle auf dem HuggingFace Hub gibt's hier:  \n",
        "https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads\n",
        "\n",
        "Wir nutzen dieses, weil es bei den Beispielen sehr gute Ergebnisse geliefert hat:  \n",
        "https://huggingface.co/Sahajtomar/German-question-answer-Electra\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_NL5XZ8rBpp"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"Sahajtomar/German-question-answer-Electra\"\n",
        "# device = 0 is GPU\n",
        "qa = pipeline(\"question-answering\", model=model_name, tokenizer=model_name, device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8goLLPuUrBpq"
      },
      "source": [
        "### Fragen zu Artikel beantworten\n",
        "\n",
        "Zun√§chst das Grundprinzip: Das Modell beantwortet Fragen basierend auf dem Kontext, z.B. ein Wikipedia-Eintrag, ein News-Artikel oder ein User-Post.\n",
        "\n",
        "Hier geht es um diesen Beispielartikel:  \n",
        "https://www.heise.de/news/Giga-Factory-Berlin-fast-fertig-Erstes-Tesla-Model-Y-noch-dieses-Jahr-6213528.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpMJX2QrBpr"
      },
      "source": [
        "context = \"\"\"Giga Factory Berlin fast fertig ‚Äì Erstes Tesla Model Y noch dieses Jahr\n",
        "\n",
        "Elon Musk hat in knapp zwei Jahren eine riesige Fabrik vor die Tore Berlins gesetzt. \n",
        "Samstag lie√ü er erstmals B√ºrger ein. Nicht alle Nachbarn sind begeistert.\n",
        "\n",
        "Der US-Elektroautobauer Tesla will sp√§testens im Dezember in Deutschland die Produktion \n",
        "f√ºr Europa starten. Dies k√ºndigte Firmengr√ºnder Elon Musk am Wochenende bei einem B√ºrgerfest \n",
        "in seinem ersten europ√§ischen Werk bei Berlin an. Kritik von Anwohnern und Umweltsch√ºtzern \n",
        "an der in nur zwei Jahren konzipierten und errichteten Industrieanlage widersprach er. \n",
        "Ziel sei \"eine wundersch√∂ne Fabrik in Harmonie mit ihrer Umgebung\".\n",
        "\n",
        "K√ºnftig sollen etwa 12.000 Mitarbeiter in Gr√ºnheide bis zu 500.000 Elektroautos im Jahr bauen. \n",
        "Dabei will Tesla m√∂glichst viele Teile vor Ort produzieren, um von Zulieferern unabh√§ngig zu sein. \n",
        "Tesla betont vor allem die Bedeutung der eigenen Druckgussanlage und der hochmodernen Lackiererei. \n",
        "Zudem entsteht neben dem Autowerk eine eigene Batteriefabrik.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aSpqF20rBps"
      },
      "source": [
        "Jetzt k√∂nnen wir Fragen stellen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqJyM2-iLu0D"
      },
      "source": [
        "question=\"Wer ist Elon Musk?\"\n",
        "answer = qa(question, context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDnwwb7rBps"
      },
      "source": [
        "question=\"Wer ist der Firmengr√ºnder?\"\n",
        "answer = qa(question, context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93EgfnUJLu8Y"
      },
      "source": [
        "question=\"Wer ist gr√ºndete Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v4227prpDnE"
      },
      "source": [
        "question=\"Was ist Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL3KiHSU9gL0"
      },
      "source": [
        "question=\"Wer ist Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGqLoyly91kd"
      },
      "source": [
        "question=\"Wer ist begeistert?\"\n",
        "# question=\"Wer ist nicht begeistert?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHXKqOG7-IXx"
      },
      "source": [
        "question=\"Wer ist der Bundeskanzler?\"\n",
        "answer = qa(question=question, context=context) # handle_impossible_answer=True, top_k=3)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr0HOWC7ceSE"
      },
      "source": [
        "question=\"Wann ist Weihnachten?\"\n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True) # False\n",
        "answer['answer']\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1iPUJgbtC_h"
      },
      "source": [
        "question=\"Wieviele Mitarbeiter?\" \n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBILyGm7-1ap"
      },
      "source": [
        "question=\"Was ist wichtig?\"\n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLpj6VOurBpu"
      },
      "source": [
        "### Fragen zu Wikipedia beantworten\n",
        "\n",
        "F√ºr einen l√§ngeren Text einen sich Wikipedia-Artikel, wie hier dieser zu \"Game of Thrones\":  \n",
        "https://de.wikipedia.org/wiki/Game_of_Thrones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR6FIQHmrBpu"
      },
      "source": [
        "from readability import Document\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "doc = Document(requests.get(\"https://de.wikipedia.org/wiki/Game_of_Thrones\", stream=True).text)\n",
        "soup = BeautifulSoup(doc.summary())            \n",
        "context = soup.text\n",
        "len(context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBd7bUKrBpv"
      },
      "source": [
        "Das sind ca. 100kB!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y80FzBM3ahg"
      },
      "source": [
        "question=\"Wer sind die Geschwister von Arya?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dohqqYhSBVPy"
      },
      "source": [
        "question=\"Wen heiratet Tyrion?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-VVEdslrBpv"
      },
      "source": [
        "question=\"Wer ist Tyrion Lennister?\"\n",
        "answer = qa(question=question, context=context) #, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i21fgE6A4i5_"
      },
      "source": [
        "question=\"Wann stirbt Eddard Stark?\"\n",
        "answer = qa(question=question, context=context, top_k=5, handle_impossible_answers=True)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJja_3Jz5mvL"
      },
      "source": [
        "question=\"Wo stirbt Eddard Stark?\"\n",
        "answer = qa(question=question, context=context, top_k=5, handle_impossible_answers=True)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeG6JoZlxxeK"
      },
      "source": [
        "## Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGtHovrx3Y4"
      },
      "source": [
        "question = \"Wie viele Menschen leben in Berlin?\"\n",
        "context = \"In Deutschland leben ca. 80 Millionen Menschen, allein in Berlin ca. 4 Mio.\"\n",
        "\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context, padding=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv9P3n1RCAju"
      },
      "source": [
        "model_name = \"Sahajtomar/German-question-answer-Electra\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV13EejOxt48"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\");\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGIkZWyzyrS"
      },
      "source": [
        "qa_df = pd.DataFrame(\n",
        "    [tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n",
        "     inputs['input_ids'][0].numpy(),\n",
        "     inputs['token_type_ids'][0].numpy(),\n",
        "     inputs['attention_mask'][0].numpy()]).T\n",
        "\n",
        "qa_df.columns=['token', 'id', 'type', 'attn']\n",
        "qa_df.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asiv5QBdyUEe"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xub8I82cBQn9"
      },
      "source": [
        "def maxval_in_col(column):    \n",
        "    highlight = 'background-color: palegreen;'\n",
        "    return [highlight if v == column.max() else '' for v in column]\n",
        "\n",
        "qa_df = pd.concat([qa_df, \n",
        "                   pd.Series(outputs['start_logits'][0].detach(), name='start'),\n",
        "                   pd.Series(outputs['end_logits'][0].detach(), name='end')], axis=1)\n",
        "\n",
        "# answer span must be in context (type==1)\n",
        "qa_df.query('type==1').style.apply(maxval_in_col, subset=['start', 'end'], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BLTSXu0Jsr"
      },
      "source": [
        "import torch\n",
        "\n",
        "start_idx = torch.argmax(outputs.start_logits)\n",
        "end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer:   {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG5eCuI8rBpx"
      },
      "source": [
        "## Retriever-Reader mit Haystack\n",
        "\n",
        "Nun wird ein gr√∂√üeres Szenario simuliert. Stellen Sie sich vor, Sie haben sehr viele Dokumente und suchen darin Antworten. Da suchen Sie die Nadel im Heuhaufen - ein Fall f√ºr [Haystack](https://haystack.deepset.ai/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCn9agtnTJBI"
      },
      "source": [
        "### Anwendungsbeispiel: Aspect-based Sentiment Analysis\n",
        "\n",
        "An dieser Stelle soll ein praktisches Anwendungsbeispiel gezeigt werden.\n",
        "Es geht darum, Kunden-Meinungen zu bestimmten Eigenschaften eines Produkts herauszufinden. Daf√ºr werden Amazon-Reviews zu dem Produkt mit einem QA-Modell \"befragt\". \n",
        "\n",
        "Da wir hier nicht nur einen (Kon-)Text auszuwerten haben, sondern viele Rewiews, wird ein Retriever-Reader-Modell benutzt. Dabei werden durch den Retriever die relevanten Kommentare vorselektiert, um dann durch den Reader ausgewertet zu werden.\n",
        "\n",
        "Der Datensatz, den wir verwenden, ist ein Auszug aus dem \"Subjective QA\" Datensatz, den man direkt vom HuggingFace Hub beziehen kann:  \n",
        "https://huggingface.co/datasets/subjqa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AI18Ny_CBmG"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# other options include: books, grocery, movies, restaurants, tripadvisor\n",
        "subjqa = load_dataset(\"subjqa\", \"electronics\")\n",
        "subjqa.set_format(\"pandas\")\n",
        "\n",
        "# flatten the nested dataset columns for easy access\n",
        "df = [ds[:] for split, ds in subjqa.flatten().items() if split == 'train'][0]\n",
        "\n",
        "# select some columns\n",
        "df = df[[\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]]\n",
        "df = df.drop_duplicates(subset=\"context\").rename(columns={\"answers.text\": \"answer\", \"answers.answer_start\": \"start\"})\n",
        "\n",
        "print(list(df.columns))\n",
        "print(f\"\\n{len(df)} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjBClQFgWQGT"
      },
      "source": [
        "Schauen wir uns ein paar Datens√§tze an:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV5cIfXPJVQr"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evl05CxFFZAV"
      },
      "source": [
        "Auswertung nach Fragetypen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPcSYzIFIrqP"
      },
      "source": [
        "counts = {}\n",
        "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
        "\n",
        "for q in question_types:\n",
        "    counts[q] = df[\"question\"].str.startswith(q).value_counts()[True]\n",
        "\n",
        "pd.Series(counts).sort_values().plot(kind=\"barh\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KVNsIdVNbE7"
      },
      "source": [
        "### Bef√ºllen des Document Stores f√ºr den Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBvuUmmaMAc0"
      },
      "source": [
        "Haystack unterst√ºtzt folgende Document Stores:\n",
        "  * Elasticsearch (Sparse BM25/TF-IDF + Dense Vectors, https://elastic.co)\n",
        "  * FAISS (von Facebook AI f√ºr Dense Vectors, https://faiss.ai/)\n",
        "  * SQL (SQLite, PostgreSQL, MySQL)\n",
        "  * InMemoryDocumentStore\n",
        "\n",
        "Der Einfachheit halber wird hier der InMemoryDocumentStore genutzt. F√ºr die Praxis wird aber ElasticSearch empfohlen, weil dieser Such-Index neben einer Volltextsuche eine Vielzahl von Filterm√∂glichkeiten f√ºr Metadaten bietet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOxRWSvL9jg"
      },
      "source": [
        "Ein Document-Store erwartet folgendes Input-Format:\n",
        "```python\n",
        "docs = [\n",
        "    {\n",
        "        'text': DOCUMENT_TEXT_HERE,\n",
        "        'meta': {'name': DOCUMENT_NAME, 'category': DOCUMENT_CATEGORY}\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbKoyzrJzPK"
      },
      "source": [
        "\n",
        "F√ºr den `InMemoryDocumentStore` wird an dieser Stelle schon einmal auf den zu analysierenden Artikel gefiltert. Wir nehmen diesen hier:\n",
        "\n",
        "**Panasonic ErgoFit In-Ear Earbud Headphones RP-HJE120-D (Orange) Dynamic Crystal Clear Sound, Ergonomic Comfort-Fit**  \n",
        "<img src=\"https://m.media-amazon.com/images/I/31oE5NluLhL.jpg\" width=\"100\"/>\n",
        "\n",
        "https://www.amazon.com/dp/B003ELYQGG  \n",
        "https://amazon-asin.com/asincheck/?product_id=B003ELYQGG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZuMKoz3OmbP"
      },
      "source": [
        "# create docs (in the example for one item only)\n",
        "item_id = \"B003ELYQGG\"\n",
        "\n",
        "docs = []\n",
        "for _, row in df.query(f\"title == '{item_id}'\").iterrows():\n",
        "    doc = {\"text\": row[\"context\"],\n",
        "           \"meta\": {\"item_id\": row[\"title\"]}}\n",
        "    docs.append(doc)\n",
        "\n",
        "docs[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdXThooNaFm"
      },
      "source": [
        "from haystack.document_store import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "document_store.write_documents(docs, index=\"document\")\n",
        "print(f\"{document_store.get_document_count()} docs loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKtMChZPS7xT"
      },
      "source": [
        "### Dokumenten-Suche mit dem Retriever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ORl6jwSZFO"
      },
      "source": [
        "from haystack.retriever.sparse import TfidfRetriever\n",
        "retriever = TfidfRetriever(document_store=document_store)\n",
        "\n",
        "question = \"How is the bass?\"\n",
        "retrieved_docs = retriever.retrieve(query=question, top_k=3)\n",
        "# ElasticSearch would support real filters\n",
        "# retrieved_docs = retriever.retrieve(query=question, top_k=3, filters={\"item_id\":[item_id]]})\n",
        "\n",
        "for doc in retrieved_docs:\n",
        "    print(fill(doc.text), end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzb2nsSqWOwB"
      },
      "source": [
        "### Antworten bekommen mit dem Reader\n",
        "\n",
        "Haystack unterst√ºtzt zwei Reader, den `FARMReader` und den `TransformersReader`. Beide nutzen Transformer-Modelle, unterscheiden sich aber in kleinen Details, die [hier](https://haystack.deepset.ai/docs/latest/readermd#deeper-dive-farm-vs-transformers) erl√§utert werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5brl7ejyWRVW"
      },
      "source": [
        "from haystack.reader.farm import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=model_name, progress_bar=False,\n",
        "                    max_seq_len=256, doc_stride=128, # these are defaults\n",
        "                    return_no_answer=False, use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D31HmRPEUymR"
      },
      "source": [
        "question = \"How is the bass?\"\n",
        "answers = reader.predict_on_texts(question=question, texts=[retrieved_docs[1].text], top_k=3)\n",
        "answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzInNa6nWjmW"
      },
      "source": [
        "Haben Sie bemerkt, dass wir immer noch das gleiche Modell verwendet haben mit dem wir auch schon die deutschen Texte analysiert haben?\n",
        "\n",
        "Das ist mit einem multilingualen Modell m√∂glich! Mit einem rein englischen Modell werden aber auf englischen Texten bessere Ergebnisse erreicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoeSGPcDsiws"
      },
      "source": [
        "### Retriever und Reader in der Haystack-Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0HTZb4uZFH0"
      },
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader, retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHp3RimuZFg8"
      },
      "source": [
        "question = \"How is the bass?\"\n",
        "# question = \"Do they sound good?\"\n",
        "# question = \"How do they fit?\"\n",
        "answers = pipe.run(query=question, params={\"Retriever\": {\"top_k\": 10}, \n",
        "                                         \"Reader\": {\"top_k\": 5}})\n",
        "\n",
        "display_qa(answers['answers'], question, padding=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6vLcWS9SwEt"
      },
      "source": [
        "### Und nat√ºrlich eine WordCloud zum Abschluss üòÄ\n",
        "\n",
        "In diesem Beispiel wird aus allen Dokumenten (wir haben nur 35) jeweils die Meinung zum Bass erfragt. Die eindeutigen Antworten werden gez√§hlt und mit einer WordCloud visualisiert. Bei sehr vielen Reviews kann man sich so sehr schnell ein Meinungsbild verschaffen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-be5qurIvnH"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "question = \"How is the bass?\"\n",
        "retrieved_docs = retriever.retrieve(query=question, top_k=100)\n",
        "\n",
        "counter = Counter()\n",
        "for doc in retrieved_docs:\n",
        "    answer = reader.predict_on_texts(question=question, texts=[doc.text], top_k=1)['answers'][0]['answer']\n",
        "    if len(answer) < 30:\n",
        "        counter.update([answer])\n",
        "\n",
        "counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkAKQ3iOJ_S"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "wc = WordCloud(width=800, height=400, background_color= \"black\", colormap=\"Paired\")\n",
        "wc.generate_from_frequencies(counter)\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}