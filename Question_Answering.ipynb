{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Question_Answering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsalbr/m3nlp/blob/main/Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiAcHrZPrBpg"
      },
      "source": [
        "# Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJxDfaburBpm"
      },
      "source": [
        "Dieses Notebook sollte mit GPU ausgeführt werden.  \n",
        "Dafür bitte zunächst im Menü \"Laufzeit\"->\"Laufzeittyp ändern\"->\"Hardwarebeschleuniger: GPU\" einstellen.\n",
        "\n",
        "\n",
        "Credits: Das Notebook verwendet Ideen von\n",
        "  * Natural Language Processing with Transformers von Lewis Tunstall, Leandro von Werra, Thomas Wolf, O'Reilly 2021, https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n",
        "  * Heise Academy NLP-Kurs von Christian Winkler, https://github.com/heiseacademy/nlp-course/tree/main/09_Transfer_Learning\n",
        "  * Haystack Tutorial von deepset.io, https://github.com/deepset-ai/haystack#mortar_board-tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtRDsDqrBpn"
      },
      "source": [
        "## System vorbereiten\n",
        "\n",
        "### Installation von Transformers und Haystack\n",
        "\n",
        "Achtung: In diesem Notebook werden sowohl die [Transformers-Bibliothek von HuggingFace](https://huggingface.co/transformers/) als auch [Haystack von deepset.ai](https://haystack.deepset.ai/) eingesetzt. \n",
        "\n",
        "Leider haben in der aktuellen Version beide Bibliotheken inkompatible Dependencies.\n",
        "Die Installation hier funktioniert für diese Beispiele, aber es gibt eine Warnung am Ende. In der Praxis kann\n",
        "es daher zu Problemen kommen. Für produktive Zwecke sollte deshalb mit getrennten\n",
        "virtuellen Environments arbeiten.\n",
        "\n",
        "**Geduld:** Die Installation dauert einen Moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbZyqQ-zqxbh"
      },
      "source": [
        "!pip install farm-haystack==0.10.0 grpcio==1.41.0\n",
        "!pip install transformers==4.12.3 datasets\n",
        "!pip install readability-lxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQD3jhTVcBVe"
      },
      "source": [
        "# patch for transformers 4.12.3, see https://github.com/huggingface/transformers/issues/14311\n",
        "!wget https://raw.githubusercontent.com/jsalbr/m3nlp/main/question_answering.patch\n",
        "!patch /usr/local/lib/python3.7/dist-packages/transformers/pipelines/question_answering.py -i question_answering.patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_Q7Cacd2ID"
      },
      "source": [
        "!patch /usr/local/lib/python3.7/dist-packages/transformers/pipelines/question_answering.py -i question_answering.patch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPTt9iVkdzo0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0JuwR2_Vui"
      },
      "source": [
        "### Noch ein paar Standard-Einstellungen setzen ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82D4tSavG7U1"
      },
      "source": [
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "from textwrap import wrap, fill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYWJh8F7T8ZY"
      },
      "source": [
        "# suppress warnings\n",
        "import warnings;\n",
        "warnings.filterwarnings('ignore');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui6-tKDZvThZ"
      },
      "source": [
        "### Und eine kleine Anzeige-Funktion ...\n",
        "\n",
        "welche mit Antworten sowohl von Transformer als auch von Haystack umgehen kann."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3SVM0ZIZcuS"
      },
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def display_qa(answers, question='', context='', padding=50):\n",
        "    if type(answers) != list:\n",
        "        answers = [answers]\n",
        "    html = \"<table>\"\n",
        "    if len(question) > 0: \n",
        "        html += f\"<tr><td>Question:</td><td><span style='font-weight:bold'>{question}</span></td></tr>\"\n",
        "        html += f\"<tr><td>&nbsp;<td><td> </td></tr>\"\n",
        "    for a in answers:\n",
        "        if len(a['answer']) > 0:\n",
        "            html += f\"<tr><td>Answer:</td><td><span style='font-weight:bold'>{a['answer']}</span></td></tr>\"\n",
        "        else:\n",
        "            html += f\"<tr><td>Answer:</td><td>answer impossible</td></tr>\"\n",
        "        html += f\"<tr><td>Score:</td><td>{a['score']}</td></tr>\"\n",
        "        start = a.get('start', a.get('offset_start'))\n",
        "        end = a.get('end', a.get('offset_end'))\n",
        "        html += f\"<tr><td>Span:</td><td>{start}:{end}</td></tr>\"\n",
        "        ctx = a.get('context', context)\n",
        "        if len(a['answer']) > 0 and len(ctx) > 0:\n",
        "            left = max(0, start-padding)\n",
        "            right = min(end+padding, len(ctx))\n",
        "            html += \"<tr><td>Snippet:</td><td>\"\n",
        "            html += f\"{ctx[left:start]}<span style='color:blue;font-weight:bold'>\"\n",
        "            html += ctx[start:end]\n",
        "            html += f\"</span>{ctx[end:right]}</td>\"\n",
        "        html += f\"<tr><td>&nbsp;<td><td> </td></tr>\"\n",
        "    html += '</table><br/>'\n",
        "    display(HTML(html))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UycdkUQf-nHR"
      },
      "source": [
        "## Arbeit mit einem QA-Modell\n",
        "\n",
        "Zunächst nutzen wir die [HuggingFace Transformers Library](https://huggingface.co/transformers/), um mit einem vortrainierten QA-Modell zu arbeiten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixsWFe1JKE8Z"
      },
      "source": [
        "### Modell laden\n",
        "\n",
        "Eine Übersicht über die QA-Modelle auf dem HuggingFace Hub gibt's hier:  \n",
        "https://huggingface.co/models?pipeline_tag=question-answering&sort=downloads\n",
        "\n",
        "Wir nutzen dieses, weil es bei den Beispielen sehr gute Ergebnisse geliefert hat:  \n",
        "https://huggingface.co/Sahajtomar/German-question-answer-Electra\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_NL5XZ8rBpp"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"Sahajtomar/German-question-answer-Electra\"\n",
        "# device = 0 is GPU\n",
        "qa = pipeline(\"question-answering\", model=model_name, tokenizer=model_name, device=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8goLLPuUrBpq"
      },
      "source": [
        "### Fragen zu Artikel beantworten\n",
        "\n",
        "Zunächst das Grundprinzip: Das Modell beantwortet Fragen basierend auf dem Kontext, z.B. ein Wikipedia-Eintrag, ein News-Artikel oder ein User-Post.\n",
        "\n",
        "Hier geht es um diesen Beispielartikel:  \n",
        "https://www.heise.de/news/Giga-Factory-Berlin-fast-fertig-Erstes-Tesla-Model-Y-noch-dieses-Jahr-6213528.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpMJX2QrBpr"
      },
      "source": [
        "context = \"\"\"Giga Factory Berlin fast fertig – Erstes Tesla Model Y noch dieses Jahr\n",
        "\n",
        "Elon Musk hat in knapp zwei Jahren eine riesige Fabrik vor die Tore Berlins gesetzt. \n",
        "Samstag ließ er erstmals Bürger ein. Nicht alle Nachbarn sind begeistert.\n",
        "\n",
        "Der US-Elektroautobauer Tesla will spätestens im Dezember in Deutschland die Produktion \n",
        "für Europa starten. Dies kündigte Firmengründer Elon Musk am Wochenende bei einem Bürgerfest \n",
        "in seinem ersten europäischen Werk bei Berlin an. Kritik von Anwohnern und Umweltschützern \n",
        "an der in nur zwei Jahren konzipierten und errichteten Industrieanlage widersprach er. \n",
        "Ziel sei \"eine wunderschöne Fabrik in Harmonie mit ihrer Umgebung\".\n",
        "\n",
        "Künftig sollen etwa 12.000 Mitarbeiter in Grünheide bis zu 500.000 Elektroautos im Jahr bauen. \n",
        "Dabei will Tesla möglichst viele Teile vor Ort produzieren, um von Zulieferern unabhängig zu sein. \n",
        "Tesla betont vor allem die Bedeutung der eigenen Druckgussanlage und der hochmodernen Lackiererei. \n",
        "Zudem entsteht neben dem Autowerk eine eigene Batteriefabrik.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aSpqF20rBps"
      },
      "source": [
        "Jetzt können wir Fragen stellen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqJyM2-iLu0D"
      },
      "source": [
        "question=\"Wer ist Elon Musk?\"\n",
        "answer = qa(question, context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBDnwwb7rBps"
      },
      "source": [
        "question=\"Wer ist der Firmengründer?\"\n",
        "answer = qa(question, context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93EgfnUJLu8Y"
      },
      "source": [
        "question=\"Wer ist gründete Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v4227prpDnE"
      },
      "source": [
        "question=\"Was ist Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL3KiHSU9gL0"
      },
      "source": [
        "question=\"Wer ist Tesla?\"\n",
        "answer = qa(question=question, context=context)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGqLoyly91kd"
      },
      "source": [
        "question=\"Wer ist begeistert?\"\n",
        "# question=\"Wer ist nicht begeistert?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHXKqOG7-IXx"
      },
      "source": [
        "question=\"Wer ist der Bundeskanzler?\"\n",
        "answer = qa(question=question, context=context) # handle_impossible_answer=True, top_k=3)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr0HOWC7ceSE"
      },
      "source": [
        "question=\"Wann ist Weihnachten?\"\n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True) # False\n",
        "answer['answer']\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1iPUJgbtC_h"
      },
      "source": [
        "question=\"Wieviele Mitarbeiter?\" \n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBILyGm7-1ap"
      },
      "source": [
        "question=\"Was ist wichtig?\"\n",
        "answer = qa(question=question, context=context, handle_impossible_answer=True, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLpj6VOurBpu"
      },
      "source": [
        "### Fragen zu Wikipedia beantworten\n",
        "\n",
        "Für einen längeren Text einen sich Wikipedia-Artikel, wie hier dieser zu \"Game of Thrones\":  \n",
        "https://de.wikipedia.org/wiki/Game_of_Thrones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR6FIQHmrBpu"
      },
      "source": [
        "from readability import Document\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "doc = Document(requests.get(\"https://de.wikipedia.org/wiki/Game_of_Thrones\", stream=True).text)\n",
        "soup = BeautifulSoup(doc.summary())            \n",
        "context = soup.text\n",
        "len(context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYBd7bUKrBpv"
      },
      "source": [
        "Das sind ca. 100kB!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y80FzBM3ahg"
      },
      "source": [
        "question=\"Wer sind die Geschwister von Arya?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dohqqYhSBVPy"
      },
      "source": [
        "question=\"Wen heiratet Tyrion?\"\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-VVEdslrBpv"
      },
      "source": [
        "question=\"Wer ist Tyrion Lennister?\"\n",
        "answer = qa(question=question, context=context) #, top_k=5)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i21fgE6A4i5_"
      },
      "source": [
        "question=\"Wann stirbt Eddard Stark?\"\n",
        "answer = qa(question=question, context=context, top_k=5, handle_impossible_answers=True)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJja_3Jz5mvL"
      },
      "source": [
        "question=\"Wo stirbt Eddard Stark?\"\n",
        "answer = qa(question=question, context=context, top_k=5, handle_impossible_answers=True)\n",
        "display_qa(answer, question, context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeG6JoZlxxeK"
      },
      "source": [
        "## Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGtHovrx3Y4"
      },
      "source": [
        "question = \"Wie viele Menschen leben in Berlin?\"\n",
        "context = \"In Deutschland leben ca. 80 Millionen Menschen, allein in Berlin ca. 4 Mio.\"\n",
        "\n",
        "answer = qa(question=question, context=context)\n",
        "display_qa(answer, question, context, padding=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv9P3n1RCAju"
      },
      "source": [
        "model_name = \"Sahajtomar/German-question-answer-Electra\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV13EejOxt48"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\");\n",
        "inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGIkZWyzyrS"
      },
      "source": [
        "qa_df = pd.DataFrame(\n",
        "    [tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n",
        "     inputs['input_ids'][0].numpy(),\n",
        "     inputs['token_type_ids'][0].numpy(),\n",
        "     inputs['attention_mask'][0].numpy()]).T\n",
        "\n",
        "qa_df.columns=['token', 'id', 'type', 'attn']\n",
        "qa_df.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asiv5QBdyUEe"
      },
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "outputs = model(**inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xub8I82cBQn9"
      },
      "source": [
        "def maxval_in_col(column):    \n",
        "    highlight = 'background-color: palegreen;'\n",
        "    return [highlight if v == column.max() else '' for v in column]\n",
        "\n",
        "qa_df = pd.concat([qa_df, \n",
        "                   pd.Series(outputs['start_logits'][0].detach(), name='start'),\n",
        "                   pd.Series(outputs['end_logits'][0].detach(), name='end')], axis=1)\n",
        "\n",
        "# answer span must be in context (type==1)\n",
        "qa_df.query('type==1').style.apply(maxval_in_col, subset=['start', 'end'], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9BLTSXu0Jsr"
      },
      "source": [
        "import torch\n",
        "\n",
        "start_idx = torch.argmax(outputs.start_logits)\n",
        "end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer:   {answer}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG5eCuI8rBpx"
      },
      "source": [
        "## Retriever-Reader mit Haystack\n",
        "\n",
        "Nun wird ein größeres Szenario simuliert. Stellen Sie sich vor, Sie haben sehr viele Dokumente und suchen darin Antworten. Da suchen Sie die Nadel im Heuhaufen - ein Fall für [Haystack](https://haystack.deepset.ai/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCn9agtnTJBI"
      },
      "source": [
        "### Anwendungsbeispiel: Aspect-based Sentiment Analysis\n",
        "\n",
        "An dieser Stelle soll ein praktisches Anwendungsbeispiel gezeigt werden.\n",
        "Es geht darum, Kunden-Meinungen zu bestimmten Eigenschaften eines Produkts herauszufinden. Dafür werden Amazon-Reviews zu dem Produkt mit einem QA-Modell \"befragt\". \n",
        "\n",
        "Da wir hier nicht nur einen (Kon-)Text auszuwerten haben, sondern viele Rewiews, wird ein Retriever-Reader-Modell benutzt. Dabei werden durch den Retriever die relevanten Kommentare vorselektiert, um dann durch den Reader ausgewertet zu werden.\n",
        "\n",
        "Der Datensatz, den wir verwenden, ist ein Auszug aus dem \"Subjective QA\" Datensatz, den man direkt vom HuggingFace Hub beziehen kann:  \n",
        "https://huggingface.co/datasets/subjqa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AI18Ny_CBmG"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# other options include: books, grocery, movies, restaurants, tripadvisor\n",
        "subjqa = load_dataset(\"subjqa\", \"electronics\")\n",
        "subjqa.set_format(\"pandas\")\n",
        "\n",
        "# flatten the nested dataset columns for easy access\n",
        "df = [ds[:] for split, ds in subjqa.flatten().items() if split == 'train'][0]\n",
        "\n",
        "# select some columns\n",
        "df = df[[\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]]\n",
        "df = df.drop_duplicates(subset=\"context\").rename(columns={\"answers.text\": \"answer\", \"answers.answer_start\": \"start\"})\n",
        "\n",
        "print(list(df.columns))\n",
        "print(f\"\\n{len(df)} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjBClQFgWQGT"
      },
      "source": [
        "Schauen wir uns ein paar Datensätze an:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV5cIfXPJVQr"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evl05CxFFZAV"
      },
      "source": [
        "Auswertung nach Fragetypen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPcSYzIFIrqP"
      },
      "source": [
        "counts = {}\n",
        "question_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n",
        "\n",
        "for q in question_types:\n",
        "    counts[q] = df[\"question\"].str.startswith(q).value_counts()[True]\n",
        "\n",
        "pd.Series(counts).sort_values().plot(kind=\"barh\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KVNsIdVNbE7"
      },
      "source": [
        "### Befüllen des Document Stores für den Retriever\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBvuUmmaMAc0"
      },
      "source": [
        "Haystack unterstützt folgende Document Stores:\n",
        "  * Elasticsearch (Sparse BM25/TF-IDF + Dense Vectors, https://elastic.co)\n",
        "  * FAISS (von Facebook AI für Dense Vectors, https://faiss.ai/)\n",
        "  * SQL (SQLite, PostgreSQL, MySQL)\n",
        "  * InMemoryDocumentStore\n",
        "\n",
        "Der Einfachheit halber wird hier der InMemoryDocumentStore genutzt. Für die Praxis wird aber ElasticSearch empfohlen, weil dieser Such-Index neben einer Volltextsuche eine Vielzahl von Filtermöglichkeiten für Metadaten bietet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxOxRWSvL9jg"
      },
      "source": [
        "Ein Document-Store erwartet folgendes Input-Format:\n",
        "```python\n",
        "docs = [\n",
        "    {\n",
        "        'text': DOCUMENT_TEXT_HERE,\n",
        "        'meta': {'name': DOCUMENT_NAME, 'category': DOCUMENT_CATEGORY}\n",
        "    }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKbKoyzrJzPK"
      },
      "source": [
        "\n",
        "Für den `InMemoryDocumentStore` wird an dieser Stelle schon einmal auf den zu analysierenden Artikel gefiltert. Wir nehmen diesen hier:\n",
        "\n",
        "**Panasonic ErgoFit In-Ear Earbud Headphones RP-HJE120-D (Orange) Dynamic Crystal Clear Sound, Ergonomic Comfort-Fit**  \n",
        "<img src=\"https://m.media-amazon.com/images/I/31oE5NluLhL.jpg\" width=\"100\"/>\n",
        "\n",
        "https://www.amazon.com/dp/B003ELYQGG  \n",
        "https://amazon-asin.com/asincheck/?product_id=B003ELYQGG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZuMKoz3OmbP"
      },
      "source": [
        "# create docs (in the example for one item only)\n",
        "item_id = \"B003ELYQGG\"\n",
        "\n",
        "docs = []\n",
        "for _, row in df.query(f\"title == '{item_id}'\").iterrows():\n",
        "    doc = {\"text\": row[\"context\"],\n",
        "           \"meta\": {\"item_id\": row[\"title\"]}}\n",
        "    docs.append(doc)\n",
        "\n",
        "docs[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpdXThooNaFm"
      },
      "source": [
        "from haystack.document_store import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore()\n",
        "\n",
        "document_store.write_documents(docs, index=\"document\")\n",
        "print(f\"{document_store.get_document_count()} docs loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKtMChZPS7xT"
      },
      "source": [
        "### Dokumenten-Suche mit dem Retriever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ORl6jwSZFO"
      },
      "source": [
        "from haystack.retriever.sparse import TfidfRetriever\n",
        "retriever = TfidfRetriever(document_store=document_store)\n",
        "\n",
        "question = \"How is the bass?\"\n",
        "retrieved_docs = retriever.retrieve(query=question, top_k=3)\n",
        "# ElasticSearch would support real filters\n",
        "# retrieved_docs = retriever.retrieve(query=question, top_k=3, filters={\"item_id\":[item_id]]})\n",
        "\n",
        "for doc in retrieved_docs:\n",
        "    print(fill(doc.text), end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zzb2nsSqWOwB"
      },
      "source": [
        "### Antworten bekommen mit dem Reader\n",
        "\n",
        "Haystack unterstützt zwei Reader, den `FARMReader` und den `TransformersReader`. Beide nutzen Transformer-Modelle, unterscheiden sich aber in kleinen Details, die [hier](https://haystack.deepset.ai/docs/latest/readermd#deeper-dive-farm-vs-transformers) erläutert werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5brl7ejyWRVW"
      },
      "source": [
        "from haystack.reader.farm import FARMReader\n",
        "\n",
        "reader = FARMReader(model_name_or_path=model_name, progress_bar=False,\n",
        "                    max_seq_len=256, doc_stride=128, # these are defaults\n",
        "                    return_no_answer=False, use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D31HmRPEUymR"
      },
      "source": [
        "question = \"How is the bass?\"\n",
        "answers = reader.predict_on_texts(question=question, texts=[retrieved_docs[1].text], top_k=3)\n",
        "answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzInNa6nWjmW"
      },
      "source": [
        "Haben Sie bemerkt, dass wir immer noch das gleiche Modell verwendet haben mit dem wir auch schon die deutschen Texte analysiert haben?\n",
        "\n",
        "Das ist mit einem multilingualen Modell möglich! Mit einem rein englischen Modell werden aber auf englischen Texten bessere Ergebnisse erreicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoeSGPcDsiws"
      },
      "source": [
        "### Retriever und Reader in der Haystack-Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0HTZb4uZFH0"
      },
      "source": [
        "from haystack.pipeline import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader, retriever)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHp3RimuZFg8"
      },
      "source": [
        "question = \"How is the bass?\"\n",
        "# question = \"Do they sound good?\"\n",
        "# question = \"How do they fit?\"\n",
        "answers = pipe.run(query=question, params={\"Retriever\": {\"top_k\": 10}, \n",
        "                                         \"Reader\": {\"top_k\": 5}})\n",
        "\n",
        "display_qa(answers['answers'], question, padding=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6vLcWS9SwEt"
      },
      "source": [
        "### Und natürlich eine WordCloud zum Abschluss 😀\n",
        "\n",
        "In diesem Beispiel wird aus allen Dokumenten (wir haben nur 35) jeweils die Meinung zum Bass erfragt. Die eindeutigen Antworten werden gezählt und mit einer WordCloud visualisiert. Bei sehr vielen Reviews kann man sich so sehr schnell ein Meinungsbild verschaffen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-be5qurIvnH"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "question = \"How is the bass?\"\n",
        "retrieved_docs = retriever.retrieve(query=question, top_k=100)\n",
        "\n",
        "counter = Counter()\n",
        "for doc in retrieved_docs:\n",
        "    answer = reader.predict_on_texts(question=question, texts=[doc.text], top_k=1)['answers'][0]['answer']\n",
        "    if len(answer) < 30:\n",
        "        counter.update([answer])\n",
        "\n",
        "counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdkAKQ3iOJ_S"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "wc = WordCloud(width=800, height=400, background_color= \"black\", colormap=\"Paired\")\n",
        "wc.generate_from_frequencies(counter)\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}